{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Data Cleaning & Integration\n",
    "\n",
    "## Objective\n",
    "Combine and clean job posting data from HackerNews and Adzuna sources.\n",
    "\n",
    "## Tasks\n",
    "- Load all raw CSV files\n",
    "- Validate data quality\n",
    "- Concatenate into single DataFrame\n",
    "- Remove duplicate postings\n",
    "- Handle missing values\n",
    "- Standardize date formats\n",
    "\n",
    "## Expected Output\n",
    "- Clean, unified dataset: `data/processed/jobs_cleaned.csv`\n",
    "- Data quality report\n",
    "- 3,500-4,000 validated job postings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T18:48:13.607689Z",
     "start_time": "2025-10-24T18:48:13.254630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully\n",
      "   Pandas version: 2.3.3\n",
      "   NumPy version: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\" Libraries imported successfully\")\n",
    "print(f\"   Pandas version: {pd.__version__}\")\n",
    "print(f\"   NumPy version: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T18:48:49.741079Z",
     "start_time": "2025-10-24T18:48:45.395372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading raw data files...\n",
      "\n",
      "======================================================================\n",
      " RAW DATA LOADED\n",
      "======================================================================\n",
      "\n",
      " Adzuna Dataset:\n",
      "   Records: 3,691\n",
      "   Columns: 19\n",
      "   Memory: 6.35 MB\n",
      "\n",
      " HackerNews Dataset:\n",
      "   Records: 711\n",
      "   Columns: 14\n",
      "   Memory: 1.22 MB\n",
      "\n",
      " Combined (before cleaning): 4,402 records\n"
     ]
    }
   ],
   "source": [
    "print(\" Loading raw data files...\\n\")\n",
    "\n",
    "df_adzuna = pd.read_csv('data/raw/adzuna_jobs_2023_2024.csv')\n",
    "df_hn = pd.read_csv('data/raw/hn_jobs_combined.csv')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" RAW DATA LOADED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Adzuna Dataset:\")\n",
    "print(f\"   Records: {len(df_adzuna):,}\")\n",
    "print(f\"   Columns: {len(df_adzuna.columns)}\")\n",
    "print(f\"   Memory: {df_adzuna.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\n HackerNews Dataset:\")\n",
    "print(f\"   Records: {len(df_hn):,}\")\n",
    "print(f\"   Columns: {len(df_hn.columns)}\")\n",
    "print(f\"   Memory: {df_hn.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\n Combined (before cleaning): {len(df_adzuna) + len(df_hn):,} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T18:48:53.123385Z",
     "start_time": "2025-10-24T18:48:53.119629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Schema Comparison:\n",
      "\n",
      "Adzuna columns:\n",
      "['job_id', 'company', 'role', 'description', 'has_ai_keywords', 'is_remote', 'location', 'salary', 'salary_min', 'salary_max', 'requires_python', 'requires_js', 'created_date', 'redirect_url', 'category', 'contract_type', 'text_length', 'source', 'scraped_date']\n",
      "\n",
      "HackerNews columns:\n",
      "['comment_id', 'company', 'role', 'description', 'has_ai_keywords', 'is_remote', 'location', 'salary', 'requires_python', 'requires_js', 'text_length', 'month', 'thread_id', 'scraped_date']\n",
      "\n",
      "Common columns:\n",
      "['company', 'description', 'has_ai_keywords', 'is_remote', 'location', 'requires_js', 'requires_python', 'role', 'salary', 'scraped_date', 'text_length']\n",
      "\n",
      "Adzuna-only columns:\n",
      "['category', 'contract_type', 'created_date', 'job_id', 'redirect_url', 'salary_max', 'salary_min', 'source']\n",
      "\n",
      "HackerNews-only columns:\n",
      "['comment_id', 'month', 'thread_id']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Schema Comparison:\\n\")\n",
    "\n",
    "print(\"Adzuna columns:\")\n",
    "print(df_adzuna.columns.tolist())\n",
    "\n",
    "print(\"\\nHackerNews columns:\")\n",
    "print(df_hn.columns.tolist())\n",
    "\n",
    "print(\"\\nCommon columns:\")\n",
    "common = set(df_adzuna.columns) & set(df_hn.columns)\n",
    "print(sorted(common))\n",
    "\n",
    "print(\"\\nAdzuna-only columns:\")\n",
    "print(sorted(set(df_adzuna.columns) - set(df_hn.columns)))\n",
    "\n",
    "print(\"\\nHackerNews-only columns:\")\n",
    "print(sorted(set(df_hn.columns) - set(df_adzuna.columns)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T18:49:10.908737Z",
     "start_time": "2025-10-24T18:49:10.879438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " DATA QUALITY REPORT: Adzuna\n",
      "======================================================================\n",
      "\n",
      "1️⃣  Missing Values:\n",
      "               Missing  Percentage\n",
      "contract_type     3446       93.36\n",
      "salary_max           2        0.05\n",
      "\n",
      "2️⃣  Data Types:\n",
      "object     11\n",
      "bool        4\n",
      "int64       2\n",
      "float64     2\n",
      "\n",
      "3️⃣  Duplicates:\n",
      "   Exact duplicates: 0 (0.00%)\n",
      "\n",
      "4️⃣  Text Field Statistics:\n",
      "   Unique companies: 1,424\n",
      "   'Not specified' companies: 0 (0.00%)\n",
      "   'Not specified' roles: 0 (0.00%)\n",
      "   Avg description length: 500 chars\n",
      "   Short descriptions (<150 chars): 0 (0.00%)\n",
      "\n",
      "======================================================================\n",
      " DATA QUALITY REPORT: HackerNews\n",
      "======================================================================\n",
      "\n",
      "1️⃣  Missing Values:\n",
      "   ✅ No missing values detected\n",
      "\n",
      "2️⃣  Data Types:\n",
      "object    7\n",
      "bool      4\n",
      "int64     3\n",
      "\n",
      "3️⃣  Duplicates:\n",
      "   Exact duplicates: 0 (0.00%)\n",
      "\n",
      "4️⃣  Text Field Statistics:\n",
      "   Unique companies: 635\n",
      "   'Not specified' companies: 0 (0.00%)\n",
      "   'Not specified' roles: 90 (12.66%)\n",
      "   Avg description length: 930 chars\n",
      "   Short descriptions (<150 chars): 0 (0.00%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing, Percentage]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assess_data_quality(df, name):\n",
    "    \"\"\"Generate comprehensive data quality report\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\" DATA QUALITY REPORT: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\n1⃣  Missing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing': missing,\n",
    "        'Percentage': missing_pct\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing'] > 0].sort_values('Missing', ascending=False)\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(missing_df.to_string())\n",
    "    else:\n",
    "        print(\"    No missing values detected\")\n",
    "    \n",
    "    print(f\"\\n2️  Data Types:\")\n",
    "    print(df.dtypes.value_counts().to_string())\n",
    "    \n",
    "    print(f\"\\n3⃣  Duplicates:\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"   Exact duplicates: {duplicates:,} ({duplicates/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n4️  Text Field Statistics:\")\n",
    "    if 'company' in df.columns:\n",
    "        print(f\"   Unique companies: {df['company'].nunique():,}\")\n",
    "        not_specified_company = (df['company'] == 'Not specified').sum()\n",
    "        print(f\"   'Not specified' companies: {not_specified_company:,} ({not_specified_company/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    if 'role' in df.columns:\n",
    "        not_specified_role = (df['role'] == 'Not specified').sum()\n",
    "        print(f\"   'Not specified' roles: {not_specified_role:,} ({not_specified_role/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    if 'description' in df.columns:\n",
    "        print(f\"   Avg description length: {df['description'].str.len().mean():.0f} chars\")\n",
    "        short_desc = (df['description'].str.len() < 150).sum()\n",
    "        print(f\"   Short descriptions (<150 chars): {short_desc:,} ({short_desc/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "assess_data_quality(df_adzuna, \"Adzuna\")\n",
    "assess_data_quality(df_hn, \"HackerNews\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HackerNews Data Filtering\n",
    "\n",
    "Remove invalid entries that were incorrectly captured during scraping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T18:51:41.062957Z",
     "start_time": "2025-10-24T18:51:41.036815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Validating HackerNews job postings...\n",
      "\n",
      "️  Invalid entries found: 100 (14.1%)\n",
      "\n",
      "Sample invalid entries:\n",
      "\n",
      "   • Company: I'd argue you're creating a\n",
      "     Role: Not specified\n",
      "     Description (first 100 chars): I'd argue you're creating a bit of a false dichotomy there. I completely agree that the public is ba...\n",
      "\n",
      "   • Company: If the ones who are\n",
      "     Role: Not specified\n",
      "     Description (first 100 chars): If the ones who are supposed to be informing the masses are deliberately feeding them information th...\n",
      "\n",
      "   • Company: worse yet, there is a\n",
      "     Role: very short sighted.\n",
      "     Description (first 100 chars): worse yet, there is a very vocal set of people (many on HN) that want to force you to develop your l...\n",
      "\n",
      " Filtered HackerNews dataset:\n",
      "   Before: 711 records\n",
      "   After: 611 records\n",
      "   Removed: 100 records (14.1%)\n"
     ]
    }
   ],
   "source": [
    "def validate_job_posting(row):\n",
    "    \"\"\"Check if a row is a valid job posting\"\"\"\n",
    "    \n",
    "    if pd.isna(row['description']) or len(str(row['description'])) < 150:\n",
    "        return False\n",
    "    \n",
    "    if row['role'] == 'Not specified' or pd.isna(row['role']):\n",
    "        return False\n",
    "    \n",
    "    if row['company'] == 'Not specified' or pd.isna(row['company']):\n",
    "        return False\n",
    "    \n",
    "    description_lower = str(row['description']).lower()\n",
    "    \n",
    "    job_keywords = [\n",
    "        'hiring', 'looking for', 'seeking', 'position', 'role', 'job',\n",
    "        'engineer', 'developer', 'designer', 'manager', 'analyst',\n",
    "        'salary', 'compensation', 'benefits', 'apply', 'resume',\n",
    "        'full-time', 'part-time', 'contract', 'remote'\n",
    "    ]\n",
    "    \n",
    "    has_job_keywords = sum(1 for kw in job_keywords if kw in description_lower)\n",
    "    \n",
    "    if has_job_keywords < 2:\n",
    "        return False\n",
    "    \n",
    "    question_starts = [\n",
    "        \"i'd argue\", \"if you\", \"does anyone\", \"can someone\",\n",
    "        \"what do you\", \"has anyone\", \"why do\", \"how do\"\n",
    "    ]\n",
    "    \n",
    "    first_100 = description_lower[:100]\n",
    "    if any(first_100.startswith(q) for q in question_starts):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"🔍 Validating HackerNews job postings...\\n\")\n",
    "\n",
    "initial_count = len(df_hn)\n",
    "\n",
    "df_hn['is_valid'] = df_hn.apply(validate_job_posting, axis=1)\n",
    "\n",
    "invalid_count = (~df_hn['is_valid']).sum()\n",
    "print(f\"️  Invalid entries found: {invalid_count:,} ({invalid_count/initial_count*100:.1f}%)\\n\")\n",
    "\n",
    "print(\"Sample invalid entries:\")\n",
    "invalid_sample = df_hn[~df_hn['is_valid']].head(3)\n",
    "for idx, row in invalid_sample.iterrows():\n",
    "    print(f\"\\n   • Company: {row['company'][:50]}\")\n",
    "    print(f\"     Role: {row['role'][:50]}\")\n",
    "    print(f\"     Description (first 100 chars): {str(row['description'])[:100]}...\")\n",
    "\n",
    "df_hn_filtered = df_hn[df_hn['is_valid']].drop('is_valid', axis=1).copy()\n",
    "\n",
    "print(f\"\\n Filtered HackerNews dataset:\")\n",
    "print(f\"   Before: {initial_count:,} records\")\n",
    "print(f\"   After: {len(df_hn_filtered):,} records\")\n",
    "print(f\"   Removed: {initial_count - len(df_hn_filtered):,} records ({(initial_count - len(df_hn_filtered))/initial_count*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Schema Standardization\n",
    "\n",
    "Align column names and add missing fields to create unified schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T18:51:51.750622Z",
     "start_time": "2025-10-24T18:51:51.738471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Standardizing schemas...\n",
      "\n",
      " Schema standardization complete\n",
      "\n",
      "   Unified columns: 21\n",
      "   Column order: alphabetical\n",
      "\n",
      "   Columns: ['category', 'company', 'contract_type', 'created_date', 'description', 'has_ai_keywords', 'id', 'is_remote', 'location', 'month', 'redirect_url', 'requires_js', 'requires_python', 'role', 'salary', 'salary_max', 'salary_min', 'scraped_date', 'source', 'text_length', 'thread_id']\n"
     ]
    }
   ],
   "source": [
    "print(\"🔧 Standardizing schemas...\\n\")\n",
    "\n",
    "df_adzuna_std = df_adzuna.copy()\n",
    "df_hn_std = df_hn_filtered.copy()\n",
    "\n",
    "df_adzuna_std.rename(columns={'job_id': 'id'}, inplace=True)\n",
    "df_hn_std.rename(columns={'comment_id': 'id'}, inplace=True)\n",
    "\n",
    "df_hn_std['id'] = 'hn_' + df_hn_std['id'].astype(str)\n",
    "df_adzuna_std['id'] = 'adz_' + df_adzuna_std['id'].astype(str)\n",
    "\n",
    "if 'source' not in df_hn_std.columns:\n",
    "    df_hn_std['source'] = 'hackernews'\n",
    "\n",
    "adzuna_only_cols = ['salary_min', 'salary_max', 'redirect_url', 'category', 'contract_type', 'created_date']\n",
    "for col in adzuna_only_cols:\n",
    "    if col not in df_hn_std.columns:\n",
    "        df_hn_std[col] = np.nan\n",
    "\n",
    "hn_only_cols = ['month', 'thread_id']\n",
    "for col in hn_only_cols:\n",
    "    if col not in df_adzuna_std.columns:\n",
    "        df_adzuna_std[col] = np.nan\n",
    "\n",
    "all_columns = sorted(set(df_adzuna_std.columns) | set(df_hn_std.columns))\n",
    "\n",
    "for col in all_columns:\n",
    "    if col not in df_adzuna_std.columns:\n",
    "        df_adzuna_std[col] = np.nan\n",
    "    if col not in df_hn_std.columns:\n",
    "        df_hn_std[col] = np.nan\n",
    "\n",
    "df_adzuna_std = df_adzuna_std[all_columns]\n",
    "df_hn_std = df_hn_std[all_columns]\n",
    "\n",
    "print(\" Schema standardization complete\\n\")\n",
    "print(f\"   Unified columns: {len(all_columns)}\")\n",
    "print(f\"   Column order: alphabetical\")\n",
    "print(f\"\\n   Columns: {all_columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Date Standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T18:52:05.179101Z",
     "start_time": "2025-10-24T18:52:05.150596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Standardizing dates...\n",
      "\n",
      " Date standardization complete\n",
      "\n",
      "Adzuna date range:\n",
      "   Min: 2025-10-21 19:26:32+00:00\n",
      "   Max: 2025-10-24 13:25:49+00:00\n",
      "   Null dates: 0\n",
      "\n",
      "HackerNews date range:\n",
      "   Min: 2024-05-01 00:00:00\n",
      "   Max: 2024-10-01 00:00:00\n",
      "   Null dates: 0\n",
      "\n",
      " Records by year-month:\n",
      "\n",
      "Adzuna:\n",
      "posting_year_month\n",
      "2025-10    3691\n",
      "\n",
      "HackerNews:\n",
      "posting_year_month\n",
      "2024-05    331\n",
      "2024-10    280\n"
     ]
    }
   ],
   "source": [
    "def standardize_dates(df, source_type):\n",
    "    \"\"\"Convert dates to unified datetime format (timezone-naive)\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if source_type == 'adzuna':\n",
    "        df['posting_date'] = pd.to_datetime(df['created_date'], errors='coerce', utc=True)\n",
    "        df['posting_date'] = df['posting_date'].dt.tz_localize(None)\n",
    "    \n",
    "    elif source_type == 'hackernews':\n",
    "        df['posting_date'] = pd.to_datetime(df['month'] + '-01', errors='coerce')\n",
    "    \n",
    "    df['posting_year'] = df['posting_date'].dt.year\n",
    "    df['posting_month'] = df['posting_date'].dt.month\n",
    "    df['posting_year_month'] = df['posting_date'].dt.to_period('M').astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\" Standardizing dates...\\n\")\n",
    "\n",
    "df_adzuna_std = standardize_dates(df_adzuna_std, 'adzuna')\n",
    "df_hn_std = standardize_dates(df_hn_std, 'hackernews')\n",
    "\n",
    "print(\" Date standardization complete\\n\")\n",
    "\n",
    "print(\"Adzuna date range:\")\n",
    "print(f\"   Min: {df_adzuna_std['posting_date'].min()}\")\n",
    "print(f\"   Max: {df_adzuna_std['posting_date'].max()}\")\n",
    "print(f\"   Null dates: {df_adzuna_std['posting_date'].isnull().sum()}\")\n",
    "\n",
    "print(\"\\nHackerNews date range:\")\n",
    "print(f\"   Min: {df_hn_std['posting_date'].min()}\")\n",
    "print(f\"   Max: {df_hn_std['posting_date'].max()}\")\n",
    "print(f\"   Null dates: {df_hn_std['posting_date'].isnull().sum()}\")\n",
    "\n",
    "print(\"\\n Records by year-month:\")\n",
    "print(\"\\nAdzuna:\")\n",
    "print(df_adzuna_std['posting_year_month'].value_counts().sort_index().head(10).to_string())\n",
    "print(\"\\nHackerNews:\")\n",
    "print(df_hn_std['posting_year_month'].value_counts().sort_index().to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combine Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T00:14:14.225359Z",
     "start_time": "2025-10-25T00:14:14.141187Z"
    }
   },
   "source": [
    "print(\"🔗 Combining datasets...\\n\")\n",
    "\n",
    "df_combined = pd.concat([df_adzuna_std, df_hn_std], ignore_index=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" COMBINED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Total records: {len(df_combined):,}\")\n",
    "print(f\"   Adzuna: {len(df_adzuna_std):,} ({len(df_adzuna_std)/len(df_combined)*100:.1f}%)\")\n",
    "print(f\"   HackerNews: {len(df_hn_std):,} ({len(df_hn_std)/len(df_combined)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n Date range: {df_combined['posting_date'].min().date()} to {df_combined['posting_date'].max().date()}\")\n",
    "print(f\"   Total span: {(df_combined['posting_date'].max() - df_combined['posting_date'].min()).days} days\")\n",
    "\n",
    "print(f\"\\n Unique companies: {df_combined['company'].nunique():,}\")\n",
    "print(f\" AI/ML jobs: {df_combined['has_ai_keywords'].sum():,} ({df_combined['has_ai_keywords'].sum()/len(df_combined)*100:.1f}%)\")\n",
    "print(f\" Remote jobs: {df_combined['is_remote'].sum():,} ({df_combined['is_remote'].sum()/len(df_combined)*100:.1f}%)\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Combining datasets...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m🔗 Combining datasets...\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m df_combined = \u001B[43mpd\u001B[49m.concat([df_adzuna_std, df_hn_std], ignore_index=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m70\u001B[39m)\n\u001B[32m      6\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m COMBINED DATASET\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'pd' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deduplication\n",
    "\n",
    "Remove duplicate job postings using multiple criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_ratio(str1, str2):\n",
    "    \"\"\"Calculate similarity between two strings\"\"\"\n",
    "    if pd.isna(str1) or pd.isna(str2):\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, str(str1).lower(), str(str2).lower()).ratio()\n",
    "\n",
    "print(\" Identifying duplicates...\\n\")\n",
    "\n",
    "initial_count = len(df_combined)\n",
    "\n",
    "print(\"Step 1: Exact duplicates (all fields)\")\n",
    "exact_dupes = df_combined.duplicated().sum()\n",
    "print(f\"   Found: {exact_dupes:,} exact duplicates\")\n",
    "df_combined = df_combined.drop_duplicates()\n",
    "print(f\"   Remaining: {len(df_combined):,}\")\n",
    "\n",
    "print(\"\\nStep 2: Duplicate IDs\")\n",
    "id_dupes = df_combined.duplicated(subset=['id']).sum()\n",
    "print(f\"   Found: {id_dupes:,} duplicate IDs\")\n",
    "df_combined = df_combined.drop_duplicates(subset=['id'])\n",
    "print(f\"   Remaining: {len(df_combined):,}\")\n",
    "\n",
    "print(\"\\nStep 3: Duplicate company + role + location\")\n",
    "business_key_dupes = df_combined.duplicated(subset=['company', 'role', 'location']).sum()\n",
    "print(f\"   Found: {business_key_dupes:,} duplicates\")\n",
    "df_combined = df_combined.drop_duplicates(subset=['company', 'role', 'location'])\n",
    "print(f\"   Remaining: {len(df_combined):,}\")\n",
    "\n",
    "print(\"\\nStep 4: Near-duplicate descriptions (company match + high similarity)\")\n",
    "print(\"   Checking for description similarity within same company...\")\n",
    "\n",
    "df_combined = df_combined.sort_values(['company', 'posting_date']).reset_index(drop=True)\n",
    "\n",
    "to_remove = set()\n",
    "companies = df_combined['company'].unique()\n",
    "\n",
    "checked = 0\n",
    "for company in companies:\n",
    "    company_jobs = df_combined[df_combined['company'] == company]\n",
    "    \n",
    "    if len(company_jobs) > 1:\n",
    "        indices = company_jobs.index.tolist()\n",
    "        descriptions = company_jobs['description'].tolist()\n",
    "        \n",
    "        for i in range(len(indices)):\n",
    "            if indices[i] in to_remove:\n",
    "                continue\n",
    "            \n",
    "            for j in range(i + 1, len(indices)):\n",
    "                if indices[j] in to_remove:\n",
    "                    continue\n",
    "                \n",
    "                sim = similarity_ratio(descriptions[i], descriptions[j])\n",
    "                \n",
    "                if sim > 0.90:\n",
    "                    to_remove.add(indices[j])\n",
    "                    checked += 1\n",
    "\n",
    "fuzzy_dupes = len(to_remove)\n",
    "print(f\"   Found: {fuzzy_dupes:,} near-duplicates (>90% similar)\")\n",
    "\n",
    "df_combined = df_combined[~df_combined.index.isin(to_remove)].reset_index(drop=True)\n",
    "print(f\"   Remaining: {len(df_combined):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" DEDUPLICATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n   Initial records: {initial_count:,}\")\n",
    "print(f\"   Final records: {len(df_combined):,}\")\n",
    "print(f\"   Total removed: {initial_count - len(df_combined):,} ({(initial_count - len(df_combined))/initial_count*100:.1f}%)\")\n",
    "print(f\"\\n   Breakdown:\")\n",
    "print(f\"     - Exact duplicates: {exact_dupes:,}\")\n",
    "print(f\"     - Duplicate IDs: {id_dupes:,}\")\n",
    "print(f\"     - Business key duplicates: {business_key_dupes:,}\")\n",
    "print(f\"     - Fuzzy description duplicates: {fuzzy_dupes:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Missing Value Handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 Handling missing values...\\n\")\n",
    "\n",
    "print(\"Missing value counts before handling:\")\n",
    "missing_before = df_combined.isnull().sum()\n",
    "missing_before = missing_before[missing_before > 0].sort_values(ascending=False)\n",
    "print(missing_before.to_string())\n",
    "\n",
    "text_fields = ['location', 'salary', 'category', 'contract_type']\n",
    "for field in text_fields:\n",
    "    if field in df_combined.columns:\n",
    "        df_combined[field] = df_combined[field].fillna('Not specified')\n",
    "\n",
    "if 'description' in df_combined.columns:\n",
    "    df_combined['description'] = df_combined['description'].fillna('')\n",
    "\n",
    "if 'redirect_url' in df_combined.columns:\n",
    "    df_combined['redirect_url'] = df_combined['redirect_url'].fillna('')\n",
    "\n",
    "critical_fields = ['company', 'role', 'source', 'posting_date']\n",
    "for field in critical_fields:\n",
    "    if field in df_combined.columns:\n",
    "        missing = df_combined[field].isnull().sum()\n",
    "        if missing > 0:\n",
    "            print(f\"\\n⚠  WARNING: {missing} records missing critical field '{field}'\")\n",
    "            print(\"   These will be removed.\")\n",
    "            df_combined = df_combined.dropna(subset=[field])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" MISSING VALUE HANDLING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nMissing value counts after handling:\")\n",
    "missing_after = df_combined.isnull().sum()\n",
    "missing_after = missing_after[missing_after > 0].sort_values(ascending=False)\n",
    "if len(missing_after) > 0:\n",
    "    print(missing_after.to_string())\n",
    "    print(\"\\n   Note: These are intentionally kept as NaN (e.g., salary data)\")\n",
    "else:\n",
    "    print(\"    No critical missing values remaining\")\n",
    "\n",
    "print(f\"\\n   Final record count: {len(df_combined):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Running final data validation...\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" FINAL VALIDATION CHECKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checks_passed = 0\n",
    "checks_total = 0\n",
    "\n",
    "checks_total += 1\n",
    "unique_ids = df_combined['id'].nunique()\n",
    "if unique_ids == len(df_combined):\n",
    "    print(f\"\\n Check 1: All IDs unique ({unique_ids:,} records)\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"\\n Check 1: FAILED - Duplicate IDs found\")\n",
    "\n",
    "checks_total += 1\n",
    "missing_critical = df_combined[['company', 'role', 'description', 'source']].isnull().any(axis=1).sum()\n",
    "if missing_critical == 0:\n",
    "    print(f\" Check 2: No missing critical fields\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\" Check 2: FAILED - {missing_critical} records missing critical fields\")\n",
    "\n",
    "checks_total += 1\n",
    "valid_dates = df_combined['posting_date'].notna().sum()\n",
    "if valid_dates == len(df_combined):\n",
    "    print(f\" Check 3: All dates valid ({valid_dates:,} records)\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\"️  Check 3: {len(df_combined) - valid_dates} invalid dates\")\n",
    "\n",
    "checks_total += 1\n",
    "valid_booleans = True\n",
    "for col in ['has_ai_keywords', 'is_remote', 'requires_python', 'requires_js']:\n",
    "    if col in df_combined.columns:\n",
    "        if df_combined[col].dtype != bool:\n",
    "            valid_booleans = False\n",
    "if valid_booleans:\n",
    "    print(f\" Check 4: Boolean columns have correct type\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\" Check 4: FAILED - Boolean columns have incorrect type\")\n",
    "\n",
    "checks_total += 1\n",
    "min_desc_length = df_combined['description'].str.len().min()\n",
    "if min_desc_length >= 0:\n",
    "    print(f\" Check 5: Description lengths valid (min: {min_desc_length} chars)\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\" Check 5: FAILED - Invalid description lengths\")\n",
    "\n",
    "checks_total += 1\n",
    "sources = df_combined['source'].unique()\n",
    "if set(sources).issubset({'adzuna', 'hackernews'}):\n",
    "    print(f\" Check 6: Valid data sources: {list(sources)}\")\n",
    "    checks_passed += 1\n",
    "else:\n",
    "    print(f\" Check 6: FAILED - Invalid sources found: {sources}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\" VALIDATION RESULT: {checks_passed}/{checks_total} checks passed\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if checks_passed == checks_total:\n",
    "    print(\"\\n All validation checks passed! Data is ready for analysis.\")\n",
    "else:\n",
    "    print(f\"\\n⚠  {checks_total - checks_passed} validation check(s) failed. Review issues above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Final Dataset Schema:\\n\")\n",
    "print(df_combined.dtypes.to_string())\n",
    "\n",
    "print(f\"\\n\\n Dataset Shape: {df_combined.shape}\")\n",
    "print(f\"   Records: {df_combined.shape[0]:,}\")\n",
    "print(f\"   Columns: {df_combined.shape[1]}\")\n",
    "print(f\"   Memory: {df_combined.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Cleaned Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'data/processed/jobs_cleaned.csv'\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "df_combined.to_csv(output_file, index=False)\n",
    "\n",
    "print(\" CLEANED DATASET SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n   File: {output_file}\")\n",
    "print(f\"   Size: {os.path.getsize(output_file) / 1024 / 1024:.2f} MB\")\n",
    "print(f\"   Records: {len(df_combined):,}\")\n",
    "print(f\"   Columns: {len(df_combined.columns)}\")\n",
    "\n",
    "print(f\"\\n Phase 2.1 Complete: Data Cleaning\")\n",
    "print(f\"\\n   Next steps: Exploratory Data Analysis (04_eda.ipynb)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Statistics & Quality Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" FINAL DATA QUALITY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1️  Dataset Size:\")\n",
    "print(f\"   Total records: {len(df_combined):,}\")\n",
    "print(f\"   Adzuna records: {(df_combined['source'] == 'adzuna').sum():,}\")\n",
    "print(f\"   HackerNews records: {(df_combined['source'] == 'hackernews').sum():,}\")\n",
    "\n",
    "print(f\"\\n️⃣  Date Coverage:\")\n",
    "print(f\"   Date range: {df_combined['posting_date'].min().date()} to {df_combined['posting_date'].max().date()}\")\n",
    "print(f\"   Total span: {(df_combined['posting_date'].max() - df_combined['posting_date'].min()).days} days\")\n",
    "print(f\"   Months covered: {df_combined['posting_year_month'].nunique()}\")\n",
    "\n",
    "print(f\"\\n3️  Companies & Roles:\")\n",
    "print(f\"   Unique companies: {df_combined['company'].nunique():,}\")\n",
    "print(f\"   Unique roles: {df_combined['role'].nunique():,}\")\n",
    "print(f\"   Unique locations: {(df_combined['location'] != 'Not specified').sum():,}\")\n",
    "\n",
    "print(f\"\\n4⃣  Job Characteristics:\")\n",
    "ai_count = df_combined['has_ai_keywords'].sum()\n",
    "print(f\"   AI/ML jobs: {ai_count:,} ({ai_count/len(df_combined)*100:.1f}%)\")\n",
    "remote_count = df_combined['is_remote'].sum()\n",
    "print(f\"   Remote jobs: {remote_count:,} ({remote_count/len(df_combined)*100:.1f}%)\")\n",
    "python_count = df_combined['requires_python'].sum()\n",
    "print(f\"   Python required: {python_count:,} ({python_count/len(df_combined)*100:.1f}%)\")\n",
    "js_count = df_combined['requires_js'].sum()\n",
    "print(f\"   JavaScript required: {js_count:,} ({js_count/len(df_combined)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n5️  Salary Information:\")\n",
    "has_salary = df_combined['salary_min'].notna().sum()\n",
    "print(f\"   Jobs with salary data: {has_salary:,} ({has_salary/len(df_combined)*100:.1f}%)\")\n",
    "if has_salary > 0:\n",
    "    print(f\"   Median min salary: ${df_combined['salary_min'].median()/1000:.0f}k\")\n",
    "    print(f\"   Median max salary: ${df_combined['salary_max'].median()/1000:.0f}k\")\n",
    "\n",
    "print(f\"\\n6⃣  Data Completeness:\")\n",
    "print(f\"   Complete company: {(df_combined['company'] != 'Not specified').sum():,} ({(df_combined['company'] != 'Not specified').sum()/len(df_combined)*100:.1f}%)\")\n",
    "print(f\"   Complete role: {(df_combined['role'] != 'Not specified').sum():,} ({(df_combined['role'] != 'Not specified').sum()/len(df_combined)*100:.1f}%)\")\n",
    "print(f\"   Complete location: {(df_combined['location'] != 'Not specified').sum():,} ({(df_combined['location'] != 'Not specified').sum()/len(df_combined)*100:.1f}%)\")\n",
    "print(f\"   Complete description: {(df_combined['description'].str.len() > 0).sum():,} ({(df_combined['description'].str.len() > 0).sum()/len(df_combined)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n7️  Top Hiring Companies (Top 10):\")\n",
    "top_companies = df_combined['company'].value_counts().head(10)\n",
    "for idx, (company, count) in enumerate(top_companies.items(), 1):\n",
    "    print(f\"   {idx:2d}. {company[:45]:<45} {count:>4} jobs\")\n",
    "\n",
    "print(f\"\\n8️  Top Locations (Top 10):\")\n",
    "top_locations = df_combined[df_combined['location'] != 'Not specified']['location'].value_counts().head(10)\n",
    "for idx, (location, count) in enumerate(top_locations.items(), 1):\n",
    "    print(f\"   {idx:2d}. {location[:45]:<45} {count:>4} jobs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" DATA CLEANING PHASE COMPLETE\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Sample cleaned records:\\n\")\n",
    "sample_cols = ['company', 'role', 'location', 'has_ai_keywords', 'is_remote', 'source', 'posting_date']\n",
    "df_combined[sample_cols].head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
