{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270c8f1ebb1e4ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T15:06:24.367955Z",
     "start_time": "2025-10-24T15:05:06.928413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ HackerNews Job Scraper - FIXED VERSION\n",
      "   Only extracts TOP-LEVEL job postings (not replies)\n",
      "\n",
      "ðŸ“Š Target: 13 months\n",
      "ðŸ“… Range: 2023-10 â†’ 2024-10\n",
      "\n",
      "Starting in 3 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2023-10 (Thread: 37739795)\n",
      "============================================================\n",
      "âœ… Extracted 2 job postings\n",
      "ðŸ’¾ Saved to data/raw/hn_jobs_2023-10.csv\n",
      "\n",
      "ðŸ“‹ Sample postings:\n",
      "   ðŸ’»ðŸ¢ I'd argue you're creating a | Not specified\n",
      "   ðŸ’»ðŸ¢ If the ones who are | Not specified\n",
      "\n",
      "ðŸ“Š Stats:\n",
      "   AI/ML roles: 0 (0.0%)\n",
      "   Remote: 0 (0.0%)\n",
      "   Python: 0\n",
      "   JavaScript: 0\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2023-11 (Thread: 38099577)\n",
      "============================================================\n",
      "âš ï¸  No job postings found\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2023-12 (Thread: 38477631)\n",
      "============================================================\n",
      "âš ï¸  No job postings found\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2024-01 (Thread: 38845878)\n",
      "============================================================\n",
      "âŒ Error fetching thread 38845878: 403 Client Error: Forbidden for url: https://news.ycombinator.com/item?id=38845878\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2024-02 (Thread: 39217462)\n",
      "============================================================\n",
      "âš ï¸  No job postings found\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2024-03 (Thread: 39563824)\n",
      "============================================================\n",
      "âœ… Extracted 1 job postings\n",
      "ðŸ’¾ Saved to data/raw/hn_jobs_2024-03.csv\n",
      "\n",
      "ðŸ“‹ Sample postings:\n",
      "   ðŸ’»ðŸ¢ worse yet, there is a | very short sighted.\n",
      "\n",
      "ðŸ“Š Stats:\n",
      "   AI/ML roles: 0 (0.0%)\n",
      "   Remote: 0 (0.0%)\n",
      "   Python: 0\n",
      "   JavaScript: 0\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2024-04 (Thread: 39895401)\n",
      "============================================================\n",
      "âš ï¸  No job postings found\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2024-05 (Thread: 40224213)\n",
      "============================================================\n",
      "âœ… Extracted 382 job postings\n",
      "ðŸ’¾ Saved to data/raw/hn_jobs_2024-05.csv\n",
      "\n",
      "ðŸ“‹ Sample postings:\n",
      "   ðŸ’»ðŸ  Internet Archive | Not specified\n",
      "   ðŸ’»ðŸ  Common Crawl Foundation | REMOTE\n",
      "   ðŸ’»ðŸ  PostHog | Full-Time\n",
      "\n",
      "ðŸ“Š Stats:\n",
      "   AI/ML roles: 91 (23.8%)\n",
      "   Remote: 245 (64.1%)\n",
      "   Python: 81\n",
      "   JavaScript: 121\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2024-06 (Thread: 40563768)\n",
      "============================================================\n",
      "âš ï¸  No job postings found\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2024-07 (Thread: 40940936)\n",
      "============================================================\n",
      "âœ… Extracted 1 job postings\n",
      "ðŸ’¾ Saved to data/raw/hn_jobs_2024-07.csv\n",
      "\n",
      "ðŸ“‹ Sample postings:\n",
      "   ðŸ’»ðŸ¢ That's not blaming socioeconom | related one. It's buried at the end of a\n",
      "\n",
      "ðŸ“Š Stats:\n",
      "   AI/ML roles: 0 (0.0%)\n",
      "   Remote: 0 (0.0%)\n",
      "   Python: 0\n",
      "   JavaScript: 0\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2024-08 (Thread: 41286547)\n",
      "============================================================\n",
      "âš ï¸  No job postings found\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2024-09 (Thread: 41425540)\n",
      "============================================================\n",
      "âš ï¸  No job postings found\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸ“… Scraping 2024-10 (Thread: 41709301)\n",
      "============================================================\n",
      "âœ… Extracted 325 job postings\n",
      "ðŸ’¾ Saved to data/raw/hn_jobs_2024-10.csv\n",
      "\n",
      "ðŸ“‹ Sample postings:\n",
      "   ðŸ’»ðŸ¢ MONUMENTAL | Not specified\n",
      "   ðŸ¤–ðŸ  Count | Senior Software Engineer\n",
      "   ðŸ’»ðŸ¢ Column ( | Not specified\n",
      "\n",
      "ðŸ“Š Stats:\n",
      "   AI/ML roles: 70 (21.5%)\n",
      "   Remote: 185 (56.9%)\n",
      "   Python: 70\n",
      "   JavaScript: 120\n",
      "â±ï¸  Waiting 5 seconds...\n",
      "\n",
      "============================================================\n",
      "ðŸŽ‰ SCRAPING COMPLETE\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š FINAL SUMMARY:\n",
      "   Total postings: 711\n",
      "   Successful months: 5/13\n",
      "   Failed months: 2023-11, 2023-12, 2024-01, 2024-02, 2024-04, 2024-06, 2024-08, 2024-09\n",
      "\n",
      "ðŸ“ˆ Monthly breakdown:\n",
      "      2023-10: 2 postings\n",
      "      2024-03: 1 postings\n",
      "      2024-05: 382 postings\n",
      "      2024-07: 1 postings\n",
      "      2024-10: 325 postings\n",
      "\n",
      "ðŸ¤– AI/ML Analysis:\n",
      "   AI-related postings: 161 (22.6%)\n",
      "\n",
      "ðŸ  Remote Work:\n",
      "   Remote postings: 430 (60.5%)\n",
      "\n",
      "ðŸ’¾ Output files:\n",
      "   â€¢ data/raw/hn_jobs_YYYY-MM.csv (individual months)\n",
      "   â€¢ data/raw/hn_jobs_combined.csv (all data)\n",
      "\n",
      "âœ… Phase 1 Complete! Ready for Phase 2 (Data Cleaning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "## Configuration\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Corrected thread IDs for Oct 2023 - Oct 2024\n",
    "HIRING_THREADS = [\n",
    "    ('2023-10', 37739795),\n",
    "    ('2023-11', 38099577),\n",
    "    ('2023-12', 38477631),\n",
    "    ('2024-01', 38845878),\n",
    "    ('2024-02', 39217462),\n",
    "    ('2024-03', 39563824),\n",
    "    ('2024-04', 39895401),\n",
    "    ('2024-05', 40224213),\n",
    "    ('2024-06', 40563768),\n",
    "    ('2024-07', 40940936),\n",
    "    ('2024-08', 41286547),\n",
    "    ('2024-09', 41425540),\n",
    "    ('2024-10', 41709301),\n",
    "]\n",
    "\n",
    "BASE_URL = 'https://news.ycombinator.com/item?id='\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Educational Research Project)'}\n",
    "\n",
    "## Helper Functions\n",
    "\n",
    "def fetch_thread(thread_id):\n",
    "    \"\"\"Fetch a HackerNews thread\"\"\"\n",
    "    url = f'{BASE_URL}{thread_id}'\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\" Error fetching thread {thread_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_job_postings(html):\n",
    "    \"\"\"Extract TOP-LEVEL job postings only (not replies)\"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Find the main comment table\n",
    "    comment_table = soup.find('table', class_='comment-tree')\n",
    "    if not comment_table:\n",
    "        print(\"     Could not find comment table\")\n",
    "        return []\n",
    "\n",
    "    # Get all table rows\n",
    "    all_rows = comment_table.find_all('tr', class_='athing comtr')\n",
    "\n",
    "    jobs = []\n",
    "    for row in all_rows:\n",
    "        # KEY FIX: Check indent level - only grab indent=0 (top-level)\n",
    "        indent_img = row.find('img', attrs={'width': True})\n",
    "        if indent_img:\n",
    "            indent_level = int(indent_img.get('width', 0))\n",
    "            # Skip if this is a reply (indent > 0)\n",
    "            if indent_level > 0:\n",
    "                continue\n",
    "\n",
    "        # Extract comment content\n",
    "        comment_div = row.find('div', class_='comment')\n",
    "        if not comment_div:\n",
    "            continue\n",
    "\n",
    "        # Get text\n",
    "        text = comment_div.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        # Filter: Must be substantial (job postings are detailed)\n",
    "        if len(text) < 150:\n",
    "            continue\n",
    "\n",
    "        # Filter: Skip if it looks like a question/reply\n",
    "        text_lower = text.lower()\n",
    "        question_indicators = [\n",
    "            'does anyone know',\n",
    "            'can someone explain',\n",
    "            'what do you think',\n",
    "            'has anyone tried',\n",
    "            '?'  # Questions usually have question marks\n",
    "        ]\n",
    "\n",
    "        # Skip if first 100 chars contain question indicators\n",
    "        first_part = text[:100].lower()\n",
    "        if any(indicator in first_part for indicator in question_indicators):\n",
    "            continue\n",
    "\n",
    "        # Parse the job posting\n",
    "        comment_id = row.get('id', '')\n",
    "        job_data = parse_job_posting(text, comment_id)\n",
    "        if job_data:\n",
    "            jobs.append(job_data)\n",
    "\n",
    "    return jobs\n",
    "\n",
    "def parse_job_posting(text, comment_id):\n",
    "    \"\"\"Parse job posting text into structured data\"\"\"\n",
    "\n",
    "    lines = text.split('\\n')\n",
    "    first_line = lines[0] if lines else ''\n",
    "\n",
    "    # Extract company name\n",
    "    company = extract_company(first_line, text)\n",
    "\n",
    "    # Extract role\n",
    "    role = extract_role(first_line, text)\n",
    "\n",
    "    # Keyword detection\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # AI/ML keywords (expanded list)\n",
    "    ai_keywords = [\n",
    "        'machine learning', 'ml engineer', 'ai engineer', 'artificial intelligence',\n",
    "        'deep learning', 'nlp', 'natural language', 'computer vision', 'llm',\n",
    "        'data scientist', 'pytorch', 'tensorflow', 'generative ai', 'gpt',\n",
    "        'neural network', 'transformers', 'reinforcement learning', 'ml ops'\n",
    "    ]\n",
    "    has_ai_keywords = any(kw in text_lower for kw in ai_keywords)\n",
    "\n",
    "    # Remote work\n",
    "    remote_keywords = ['remote', 'work from home', 'wfh', 'distributed', 'anywhere']\n",
    "    is_remote = any(kw in text_lower for kw in remote_keywords)\n",
    "\n",
    "    # Location\n",
    "    location = extract_location(text)\n",
    "\n",
    "    # Salary\n",
    "    salary = extract_salary(text)\n",
    "\n",
    "    # Extract requirements/skills\n",
    "    requires_python = 'python' in text_lower\n",
    "    requires_js = any(x in text_lower for x in ['javascript', 'typescript', 'react', 'node.js'])\n",
    "\n",
    "    return {\n",
    "        'comment_id': comment_id,\n",
    "        'company': company,\n",
    "        'role': role,\n",
    "        'description': text[:1500],  # First 1500 chars\n",
    "        'has_ai_keywords': has_ai_keywords,\n",
    "        'is_remote': is_remote,\n",
    "        'location': location,\n",
    "        'salary': salary,\n",
    "        'requires_python': requires_python,\n",
    "        'requires_js': requires_js,\n",
    "        'text_length': len(text)\n",
    "    }\n",
    "\n",
    "def extract_company(first_line, full_text):\n",
    "    \"\"\"Extract company name from posting\"\"\"\n",
    "    # Common patterns: \"Company | Role\" or \"Company - Role\" or just \"Company\"\n",
    "\n",
    "    # Try pattern with separator\n",
    "    for sep in ['|', '-', 'â€“']:\n",
    "        if sep in first_line:\n",
    "            company = first_line.split(sep)[0].strip()\n",
    "            # Clean it up\n",
    "            company = re.sub(r'\\([^)]*\\)', '', company)  # Remove parentheses\n",
    "            company = company.strip()\n",
    "            if 2 < len(company) < 100:\n",
    "                return company\n",
    "\n",
    "    # Take first few words\n",
    "    words = first_line.split()[:5]\n",
    "    company = ' '.join(words)\n",
    "    company = re.sub(r'\\([^)]*\\)', '', company)\n",
    "    return company.strip()[:100]\n",
    "\n",
    "def extract_role(first_line, full_text):\n",
    "    \"\"\"Extract job role\"\"\"\n",
    "    # Look after | or -\n",
    "    for sep in ['|', '-', 'â€“']:\n",
    "        if sep in first_line:\n",
    "            parts = first_line.split(sep)\n",
    "            if len(parts) > 1:\n",
    "                role = parts[1].strip()\n",
    "                if 5 < len(role) < 150:\n",
    "                    return role\n",
    "\n",
    "    # Look for keywords\n",
    "    role_patterns = [\n",
    "        r'(?:seeking|hiring|looking for)[:\\s]+([^\\n]+?)(?:\\n|$)',\n",
    "        r'(?:position|role)[:\\s]+([^\\n]+?)(?:\\n|$)',\n",
    "    ]\n",
    "\n",
    "    for pattern in role_patterns:\n",
    "        match = re.search(pattern, full_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            role = match.group(1).strip()\n",
    "            if 5 < len(role) < 150:\n",
    "                return role\n",
    "\n",
    "    return \"Not specified\"\n",
    "\n",
    "def extract_location(text):\n",
    "    \"\"\"Extract location\"\"\"\n",
    "    patterns = [\n",
    "        r'(?:location|based in|office)[:\\s]+([^\\n|]+?)(?:\\n|$)',\n",
    "        r'\\b([A-Z][a-z]+,\\s*[A-Z]{2})\\b',  # City, ST\n",
    "        r'\\b(San Francisco|New York|Seattle|Austin|Boston|London|Berlin|Toronto)\\b'\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()[:100]\n",
    "\n",
    "    return \"Not specified\"\n",
    "\n",
    "def extract_salary(text):\n",
    "    \"\"\"Extract salary if mentioned\"\"\"\n",
    "    patterns = [\n",
    "        r'\\$[\\d,]+k?(?:\\s*[-â€“]\\s*\\$?[\\d,]+k?)?',\n",
    "        r'[\\d,]+k\\s*[-â€“]\\s*[\\d,]+k',\n",
    "        r'â‚¬[\\d,]+k?(?:\\s*[-â€“]\\s*â‚¬?[\\d,]+k?)?'\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "\n",
    "    return \"Not specified\"\n",
    "\n",
    "## Main Scraping Function\n",
    "\n",
    "def scrape_hiring_thread(month, thread_id):\n",
    "    \"\"\"Scrape a single hiring thread\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" Scraping {month} (Thread: {thread_id})\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    html = fetch_thread(thread_id)\n",
    "    if not html:\n",
    "        return None\n",
    "\n",
    "    jobs = extract_job_postings(html)\n",
    "\n",
    "    if not jobs:\n",
    "        print(f\"  No job postings found\")\n",
    "        return None\n",
    "\n",
    "    print(f\" Extracted {len(jobs)} job postings\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(jobs)\n",
    "    df['month'] = month\n",
    "    df['thread_id'] = thread_id\n",
    "    df['scraped_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Save to CSV\n",
    "    filename = f'data/raw/hn_jobs_{month}.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\" Saved to {filename}\")\n",
    "\n",
    "    # Show samples\n",
    "    print(f\"\\n Sample postings:\")\n",
    "    for idx, row in df.head(3).iterrows():\n",
    "        ai_flag = \"\" if row['has_ai_keywords'] else \"\"\n",
    "        remote_flag = \"\" if row['is_remote'] else \"\"\n",
    "        print(f\"   {ai_flag}{remote_flag} {row['company'][:30]} | {row['role'][:40]}\")\n",
    "\n",
    "    print(f\"\\n Stats:\")\n",
    "    print(f\"   AI/ML roles: {df['has_ai_keywords'].sum()} ({df['has_ai_keywords'].sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Remote: {df['is_remote'].sum()} ({df['is_remote'].sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Python: {df['requires_python'].sum()}\")\n",
    "    print(f\"   JavaScript: {df['requires_js'].sum()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "## Execute Scraping\n",
    "\n",
    "print(\" HackerNews Job Scraper - FIXED VERSION\")\n",
    "print(\"   Only extracts TOP-LEVEL job postings (not replies)\")\n",
    "print(f\"\\n Target: {len(HIRING_THREADS)} months\")\n",
    "print(f\" Range: {HIRING_THREADS[0][0]} â†’ {HIRING_THREADS[-1][0]}\")\n",
    "\n",
    "# Ask for confirmation\n",
    "print(\"\\nStarting in 3 seconds...\")\n",
    "time.sleep(3)\n",
    "\n",
    "all_data = []\n",
    "failed = []\n",
    "\n",
    "for month, thread_id in HIRING_THREADS:\n",
    "    df = scrape_hiring_thread(month, thread_id)\n",
    "    if df is not None:\n",
    "        all_data.append(df)\n",
    "    else:\n",
    "        failed.append(month)\n",
    "\n",
    "    # Rate limiting\n",
    "    print(\"â±  Waiting 5 seconds...\")\n",
    "    time.sleep(5)\n",
    "\n",
    "## Final Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" SCRAPING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if all_data:\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df.to_csv('data/raw/hn_jobs_combined.csv', index=False)\n",
    "\n",
    "    print(f\"\\n FINAL SUMMARY:\")\n",
    "    print(f\"   Total postings: {len(combined_df):,}\")\n",
    "    print(f\"   Successful months: {len(all_data)}/{len(HIRING_THREADS)}\")\n",
    "    if failed:\n",
    "        print(f\"   Failed months: {', '.join(failed)}\")\n",
    "\n",
    "    print(f\"\\n Monthly breakdown:\")\n",
    "    monthly = combined_df.groupby('month').size().sort_index()\n",
    "    for m, count in monthly.items():\n",
    "        print(f\"      {m}: {count:,} postings\")\n",
    "\n",
    "    print(f\"\\n AI/ML Analysis:\")\n",
    "    ai_count = combined_df['has_ai_keywords'].sum()\n",
    "    print(f\"   AI-related postings: {ai_count:,} ({ai_count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "    print(f\"\\n Remote Work:\")\n",
    "    remote_count = combined_df['is_remote'].sum()\n",
    "    print(f\"   Remote postings: {remote_count:,} ({remote_count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "    print(f\"\\n Output files:\")\n",
    "    print(f\"   â€¢ data/raw/hn_jobs_YYYY-MM.csv (individual months)\")\n",
    "    print(f\"   â€¢ data/raw/hn_jobs_combined.csv (all data)\")\n",
    "\n",
    "    print(\"\\n Phase 1 Complete! Ready for Phase 2 (Data Cleaning)\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n No data collected\")\n",
    "    print(\"   Check thread IDs and internet connection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
